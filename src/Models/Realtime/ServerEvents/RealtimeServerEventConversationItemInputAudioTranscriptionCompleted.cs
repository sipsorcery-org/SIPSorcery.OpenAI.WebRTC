using System.Collections.Generic;
using System.Text.Json.Serialization;

namespace SIPSorcery.OpenAIWebRTC.Models;

/// <summary>
/// Emitted when transcription completes for audio committed to the input buffer.
/// This occurs asynchronously, and may arrive before or after related response events.
/// The transcription is generated by an ASR model (currently always Whisper-1),
/// and may differ from how the model interprets the audio.
/// </summary>
public class RealtimeServerEventConversationItemInputAudioTranscriptionCompleted : RealtimeEventBase
{
    /// <summary>
    /// The fixed event type value:
    /// "conversation.item.input_audio_transcription.completed".
    /// </summary>
    public const string TypeName = "conversation.item.input_audio_transcription.completed";

    /// <summary>
    /// Overrides the base type property with the constant event type.
    /// </summary>
    [JsonPropertyName("type")]
    public override string? Type => TypeName;

    /// <summary>
    /// The ID of the user message item containing the audio.
    /// </summary>
    [JsonPropertyName("item_id")]
    public required string ItemID { get; set; }

    /// <summary>
    /// The index of the content part containing the audio.
    /// </summary>
    [JsonPropertyName("content_index")]
    public required int ContentIndex { get; set; }

    /// <summary>
    /// The transcription result of the input audio.
    /// </summary>
    [JsonPropertyName("transcript")]
    public required string Transcript { get; set; }

    /// <summary>
    /// Optional log probabilities of the transcription result, if available.
    /// </summary>
    [JsonPropertyName("logprobs")]
    public List<RealtimeLogProbProperties>? LogProbs { get; set; }
}
